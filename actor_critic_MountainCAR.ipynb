{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "from collections import namedtuple, deque"
      ],
      "metadata": {
        "id": "xxzLXZ5DBuS3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nog4cf4hAR8G"
      },
      "outputs": [],
      "source": [
        "#Network Model\n",
        "\n",
        "def hidden_init(layer):\n",
        "    fan_in = layer.weight.data.size()[0]\n",
        "    lim = 1. / np.sqrt(fan_in)\n",
        "    return (-lim, lim)\n",
        "\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    \"\"\"Actor (Policy) Model.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed, fc1_units=5, fc2_units=5):\n",
        "        \"\"\"Initialize parameters and build model.\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): Dimension of each state\n",
        "            action_size (int): Dimension of each action\n",
        "            seed (int): Random seed\n",
        "            fc1_units (int): Number of nodes in first hidden layer\n",
        "            fc2_units (int): Number of nodes in second hidden layer\n",
        "        \"\"\"\n",
        "        super(Actor, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        \n",
        "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
        "        self.ln1 = nn.LayerNorm(fc1_units)\n",
        "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
        "        self.ln2 = nn.LayerNorm(fc2_units)\n",
        "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
        "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
        "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
        "\n",
        "    def forward(self, state):\n",
        "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
        "        x = state\n",
        "        x = self.fc1(x)\n",
        "        x = self.ln1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.ln2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        return torch.tanh(x)\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    \"\"\"Critic (Value) Model.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed, fc1_units=20, fc2_units=10):\n",
        "        \"\"\"Initialize parameters and build model.\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): Dimension of each state\n",
        "            action_size (int): Dimension of each action\n",
        "            seed (int): Random seed\n",
        "            fcs1_units (int): Number of nodes in the first hidden layer\n",
        "            fc2_units (int): Number of nodes in the second hidden layer\n",
        "            fc3_units (int): Number of nodes in the third hidden layer\n",
        "        \"\"\"\n",
        "        super(Critic, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
        "        self.bn1 = nn.BatchNorm1d(fc1_units)\n",
        "        self.fc2 = nn.Linear(fc1_units+action_size, fc2_units)\n",
        "        self.fc3 = nn.Linear(fc2_units, 1)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
        "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
        "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
        "        xs = self.fc1(state)\n",
        "        xs = self.bn1(xs)\n",
        "        xs = F.leaky_relu(xs)\n",
        "        x = torch.cat((xs, action), dim=1)\n",
        "        x = self.fc2(x)\n",
        "        x = F.leaky_relu(x)\n",
        "        return self.fc3(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Agent\n",
        "\n",
        "BUFFER_SIZE = int(1e6)  # replay buffer size\n",
        "BATCH_SIZE = 64         # minibatch size\n",
        "GAMMA = 0.99            # discount factor\n",
        "TAU = 1e-3              # for soft update of target parameters\n",
        "LR_ACTOR = 1e-2         # learning rate of the actor\n",
        "LR_CRITIC = 5e-3        # learning rate of the critic\n",
        "WEIGHT_DECAY = 0        # L2 weight decay\n",
        "EPSILON_MAX = 1.0\n",
        "EPSILON_MIN = 0.1\n",
        "EPSILON_DECAY = 1e-6\n",
        "LEARN_START = 20000\n",
        "UPDATE_EVERY = 1\n",
        "UPDATES_PER_STEP = 1\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Agent():\n",
        "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size, random_seed):\n",
        "        \"\"\"Initialize an Agent object.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): dimension of each state\n",
        "            action_size (int): dimension of each action\n",
        "            random_seed (int): random seed\n",
        "        \"\"\"\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.seed = random.seed(random_seed)\n",
        "        self.epsilon = EPSILON_MAX\n",
        "\n",
        "        # Actor Network (w/ Target Network)\n",
        "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
        "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
        "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
        "\n",
        "        # Critic Network (w/ Target Network)\n",
        "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
        "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
        "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "        # Noise process\n",
        "        self.noise = OUNoise(action_size, random_seed, mu=0, theta=0.15, sigma=0.2)\n",
        "\n",
        "        # Replay memory\n",
        "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
        "\n",
        "        # Make sure target is with the same weight as the source\n",
        "        self.hard_update(self.actor_target, self.actor_local)\n",
        "        self.hard_update(self.critic_target, self.critic_local)\n",
        "\n",
        "        self.t_step = 0\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done, timestep):\n",
        "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
        "        # Save experience / reward        \n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "        if len(self.memory) > LEARN_START:\n",
        "            # Learn every UPDATE_EVERY time steps.\n",
        "            self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
        "            if self.t_step == 0:\n",
        "                # Learn, if enough samples are available in memory\n",
        "                if len(self.memory) > BATCH_SIZE:\n",
        "                    for _ in range(UPDATES_PER_STEP):\n",
        "                        experiences = self.memory.sample()\n",
        "                        self.learn(experiences, GAMMA)\n",
        "\n",
        "    def act(self, state, add_noise):\n",
        "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
        "\n",
        "        state = torch.from_numpy(state).float().to(device)\n",
        "\n",
        "        self.actor_local.eval()\n",
        "        with torch.no_grad():\n",
        "            action = self.actor_local(state).cpu().data.numpy()\n",
        "\n",
        "        self.actor_local.train()\n",
        "\n",
        "        if add_noise:\n",
        "            action += self.epsilon * self.noise.sample()\n",
        "\n",
        "        return np.clip(action, -1, 1)\n",
        "\n",
        "    def reset(self):\n",
        "        self.noise.reset()\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
        "        Q_targets = r + ? * critic_target(next_state, actor_target(next_state))\n",
        "        where:\n",
        "            actor_target(state) -> action\n",
        "            critic_target(state, action) -> Q-value\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples\n",
        "            gamma (float): discount factor\n",
        "        \"\"\"\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        # ---------------------------- update critic ---------------------------- #\n",
        "        # Get predicted next-state actions and Q values from target models\n",
        "        actions_next = self.actor_target(next_states)\n",
        "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
        "\n",
        "        # Compute Q targets for current states (y_i)\n",
        "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "\n",
        "        # Compute critic loss\n",
        "        Q_expected = self.critic_local(states, actions)\n",
        "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
        "\n",
        "        # Minimize the loss\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        #torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), 1)\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        # ---------------------------- update actor ---------------------------- #\n",
        "        # Compute actor loss\n",
        "        actions_pred = self.actor_local(states)\n",
        "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
        "\n",
        "        # Minimize the loss\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # ----------------------- update target networks ----------------------- #\n",
        "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
        "        self.soft_update(self.actor_local, self.actor_target, TAU)\n",
        "\n",
        "        # ---------------------------- update noise ---------------------------- #\n",
        "        if self.epsilon - EPSILON_DECAY > EPSILON_MIN:\n",
        "            self.epsilon -= EPSILON_DECAY\n",
        "        else:\n",
        "            self.epsilon = EPSILON_MIN\n",
        "\n",
        "        self.noise.reset()\n",
        "\n",
        "    def soft_update(self, local_model, target_model, tau):\n",
        "        \"\"\"Soft update model parameters.\n",
        "        ?_target = t*?_local + (1 - t)*?_target\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            local_model: PyTorch model (weights will be copied from)\n",
        "            target_model: PyTorch model (weights will be copied to)\n",
        "            tau (float): interpolation parameter\n",
        "        \"\"\"\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
        "\n",
        "    def hard_update(self, target, source):\n",
        "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
        "            target_param.data.copy_(param.data)\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
        "\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "        \"\"\"Initialize a ReplayBuffer object.\n",
        "        Params\n",
        "        ======\n",
        "            buffer_size (int): maximum size of buffer\n",
        "            batch_size (int): size of each training batch\n",
        "        \"\"\"\n",
        "        self.action_size = action_size\n",
        "        self.buffer_size = buffer_size\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def reset(self):\n",
        "        self.memory = deque(maxlen=self.buffer_size)\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "TO3XGeU-Cc53"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Noise\n",
        "\n",
        "class OUNoise:\n",
        "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
        "\n",
        "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
        "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
        "        self.mu = mu * np.ones(size)\n",
        "        self.theta = theta\n",
        "        self.sigma = sigma\n",
        "        self.seed = random.seed(seed)\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
        "        self.state = copy.copy(self.mu)\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
        "        x = self.state\n",
        "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
        "        self.state = x + dx\n",
        "        return self.state"
      ],
      "metadata": {
        "id": "lOrHXwQvBkKx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym.spaces\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "from itertools import count\n",
        "import time\n",
        "\n",
        "module_path = os.path.abspath(os.path.join('../..'))\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device: \", device)"
      ],
      "metadata": {
        "id": "prRyrvS4CuOP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cccbadd3-4847-4ea9-c0f9-113922021d5c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_seed = 2\n",
        "\n",
        "env = gym.make('MountainCarContinuous-v0')\n",
        "env.seed(random_seed)\n",
        "\n",
        "# size of each action\n",
        "action_size = env.action_space.shape[0]\n",
        "print('Size of each action:', action_size)\n",
        "\n",
        "# examine the state space \n",
        "state_size = env.observation_space.shape[0]\n",
        "print('Size of state:', state_size)\n",
        "\n",
        "action_low = env.action_space.low\n",
        "print('Action low:', action_low)\n",
        "\n",
        "action_high = env.action_space.high\n",
        "print('Action high: ', action_high)"
      ],
      "metadata": {
        "id": "h4tEzNCODzg6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa5d26d0-7ead-408e-a013-956c44fd17d0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of each action: 1\n",
            "Size of state: 2\n",
            "Action low: [-1.]\n",
            "Action high:  [1.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:318: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:257: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  \"Function `env.seed(seed)` is marked as deprecated and will be removed in the future. \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(state_size=state_size, action_size=action_size, random_seed=random_seed)\n"
      ],
      "metadata": {
        "id": "w84wdpvVE91u"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(actor_path,critic_path):\n",
        "    print(\"Model Save...\")\n",
        "    torch.save(agent.actor_local.state_dict(), actor_path)\n",
        "    torch.save(agent.critic_local.state_dict(), critic_path)"
      ],
      "metadata": {
        "id": "rFKE_V_c8rik"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ddpg(n_episodes=1000, max_t=500, print_every=1, save_every=50,b_noise=False,actor_path='cp_a.pth',critic_path='cp_c.pth'):\n",
        "    scores_deque = deque(maxlen=print_every)\n",
        "    scores = []\n",
        "    \n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        state = env.reset()\n",
        "        agent.reset()\n",
        "        score = 0\n",
        "        timestep = time.time()\n",
        "        for t in range(max_t):\n",
        "            action = agent.act(state,b_noise)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.step(state, action, reward, next_state, done, t)\n",
        "            score += reward\n",
        "            state = next_state            \n",
        "            if done:\n",
        "                break \n",
        "                \n",
        "        scores_deque.append(score)\n",
        "        scores.append(score)\n",
        "        score_average = np.mean(scores_deque)\n",
        "        \n",
        "        if i_episode % save_every == 0:\n",
        "            save_model(actor_path,critic_path)\n",
        "        \n",
        "        if i_episode % print_every == 0:\n",
        "            print('\\rEpisode {}, Average Score: {:.2f}, Max: {:.2f}, Min: {:.2f}, Time: {:.2f}'\\\n",
        "                  .format(i_episode, score_average, np.max(scores), np.min(scores), time.time() - timestep), end=\"\\n\")\n",
        "                    \n",
        "        if np.mean(scores_deque) >= 50.0:            \n",
        "            save_model(actor_path,critic_path)\n",
        "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, score_average))            \n",
        "            break            \n",
        "            \n",
        "            \n",
        "    return scores\n",
        "\n",
        "scores = ddpg(n_episodes=100000, max_t=50000, print_every=1, save_every=100,b_noise=False,actor_path='cp_actorM1.pth',critic_path='cp_criticM1.pth')\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "plt.plot(np.arange(1, len(scores)+1), scores)\n",
        "plt.ylabel('Score_without_Noise')\n",
        "plt.xlabel('Episode #')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        },
        "id": "71MQkIp2GIQx",
        "outputId": "68f0eb7b-6a75-483e-bc87-440407500c4a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1, Average Score: -5.45, Max: -5.45, Min: -5.45, Time: 4.61\n",
            "Episode 2, Average Score: -5.45, Max: -5.45, Min: -5.45, Time: 0.80\n",
            "Episode 3, Average Score: -5.45, Max: -5.45, Min: -5.45, Time: 0.99\n",
            "Episode 4, Average Score: -5.45, Max: -5.45, Min: -5.45, Time: 0.93\n",
            "Episode 5, Average Score: -5.45, Max: -5.45, Min: -5.45, Time: 0.88\n",
            "Episode 6, Average Score: -5.45, Max: -5.45, Min: -5.45, Time: 0.45\n",
            "Episode 7, Average Score: -5.45, Max: -5.45, Min: -5.45, Time: 0.47\n",
            "Episode 8, Average Score: -5.45, Max: -5.45, Min: -5.45, Time: 0.46\n",
            "Episode 9, Average Score: -5.45, Max: -5.45, Min: -5.45, Time: 0.46\n",
            "Episode 10, Average Score: -5.45, Max: -5.45, Min: -5.45, Time: 0.46\n",
            "Episode 11, Average Score: -5.45, Max: -5.45, Min: -5.45, Time: 0.46\n",
            "Episode 12, Average Score: -5.45, Max: -5.45, Min: -5.45, Time: 0.46\n",
            "Episode 13, Average Score: -5.45, Max: -5.45, Min: -5.45, Time: 0.46\n",
            "Episode 14, Average Score: -5.45, Max: -5.45, Min: -5.45, Time: 0.45\n",
            "Episode 15, Average Score: -5.45, Max: -5.45, Min: -5.45, Time: 0.46\n",
            "Episode 16, Average Score: -5.45, Max: -5.45, Min: -5.45, Time: 0.50\n",
            "Episode 17, Average Score: -5.45, Max: -5.45, Min: -5.45, Time: 0.44\n",
            "Episode 18, Average Score: -5.45, Max: -5.45, Min: -5.45, Time: 0.46\n",
            "Episode 19, Average Score: -5.45, Max: -5.45, Min: -5.45, Time: 0.44\n",
            "Episode 20, Average Score: -5.45, Max: -5.45, Min: -5.45, Time: 0.46\n",
            "Episode 21, Average Score: 50.04, Max: 50.04, Min: -5.45, Time: 6.52\n",
            "Model Save...\n",
            "\n",
            "Environment solved in 21 episodes!\tAverage Score: 50.04\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DDPG with noise"
      ],
      "metadata": {
        "id": "XuW__-ojBta0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ddpg_noise(n_episodes=1000, max_t=500, print_every=1, save_every=50,b_noise=True,actor_path='cp_a.pth',critic_path='cp_c.pth'):\n",
        "    scores_deque = deque(maxlen=print_every)\n",
        "    scores = []\n",
        "    \n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        state = env.reset()\n",
        "        agent.reset()\n",
        "        score = 0\n",
        "        timestep = time.time()\n",
        "        for t in range(max_t):\n",
        "            action = agent.act(state,b_noise)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.step(state, action, reward, next_state, done, t)\n",
        "            score += reward\n",
        "            state = next_state            \n",
        "            if done:\n",
        "                break \n",
        "                \n",
        "        scores_deque.append(score)\n",
        "        scores.append(score)\n",
        "        score_average = np.mean(scores_deque)\n",
        "        \n",
        "        if i_episode % save_every == 0:\n",
        "            save_model(actor_path,critic_path)\n",
        "        \n",
        "        if i_episode % print_every == 0:\n",
        "            print('\\rEpisode {}, Average Score: {:.2f}, Max: {:.2f}, Min: {:.2f}, Time: {:.2f}'\\\n",
        "                  .format(i_episode, score_average, np.max(scores), np.min(scores), time.time() - timestep), end=\"\\n\")\n",
        "                    \n",
        "        if np.mean(scores_deque) >= 50.0:            \n",
        "            save_model(actor_path,critic_path)\n",
        "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, score_average))            \n",
        "            break            \n",
        "            \n",
        "            \n",
        "    return scores\n",
        "\n",
        "scores2 = ddpg_noise(n_episodes=100000, max_t=50000, print_every=1, save_every=100,b_noise=True,actor_path='cp_actorM2.pth',critic_path='cp_criticM2.pth')\n",
        "\n",
        "fig2 = plt.figure()\n",
        "ax2 = fig2.add_subplot(111)\n",
        "plt.plot(np.arange(1, len(scores2)+1), scores2)\n",
        "plt.ylabel('Score_wit_Noise')\n",
        "plt.xlabel('Episode #')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 871
        },
        "id": "Bq1D9bFkBsbs",
        "outputId": "17e8e209-8a00-4259-a0f8-14ea5967dcd0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1, Average Score: -7.16, Max: -7.16, Min: -7.16, Time: 7.63\n",
            "Episode 2, Average Score: -8.19, Max: -7.16, Min: -8.19, Time: 7.57\n",
            "Episode 3, Average Score: -20.47, Max: -7.16, Min: -20.47, Time: 12.83\n",
            "Episode 4, Average Score: -6.05, Max: -6.05, Min: -20.47, Time: 7.39\n",
            "Episode 5, Average Score: -47.41, Max: -6.05, Min: -47.41, Time: 7.49\n",
            "Episode 6, Average Score: -59.98, Max: -6.05, Min: -59.98, Time: 12.36\n",
            "Episode 7, Average Score: -58.95, Max: -6.05, Min: -59.98, Time: 7.37\n",
            "Episode 8, Average Score: -99.89, Max: -6.05, Min: -99.89, Time: 7.44\n",
            "Episode 9, Average Score: -79.60, Max: -6.05, Min: -99.89, Time: 7.41\n",
            "Episode 10, Average Score: -35.34, Max: -6.05, Min: -99.89, Time: 7.30\n",
            "Episode 11, Average Score: -23.08, Max: -6.05, Min: -99.89, Time: 7.34\n",
            "Episode 12, Average Score: -16.62, Max: -6.05, Min: -99.89, Time: 8.28\n",
            "Episode 13, Average Score: -53.78, Max: -6.05, Min: -99.89, Time: 7.41\n",
            "Episode 14, Average Score: -66.62, Max: -6.05, Min: -99.89, Time: 8.18\n",
            "Episode 15, Average Score: -81.23, Max: -6.05, Min: -99.89, Time: 7.42\n",
            "Episode 16, Average Score: -81.19, Max: -6.05, Min: -99.89, Time: 7.52\n",
            "Episode 17, Average Score: -80.83, Max: -6.05, Min: -99.89, Time: 7.41\n",
            "Episode 18, Average Score: -81.56, Max: -6.05, Min: -99.89, Time: 7.41\n",
            "Episode 19, Average Score: -82.16, Max: -6.05, Min: -99.89, Time: 8.19\n",
            "Episode 20, Average Score: -81.13, Max: -6.05, Min: -99.89, Time: 7.53\n",
            "Episode 21, Average Score: -81.19, Max: -6.05, Min: -99.89, Time: 7.45\n",
            "Episode 22, Average Score: -82.05, Max: -6.05, Min: -99.89, Time: 7.47\n",
            "Episode 23, Average Score: -81.75, Max: -6.05, Min: -99.89, Time: 7.39\n",
            "Episode 24, Average Score: -81.12, Max: -6.05, Min: -99.89, Time: 7.51\n",
            "Episode 25, Average Score: -81.92, Max: -6.05, Min: -99.89, Time: 7.45\n",
            "Episode 26, Average Score: -81.75, Max: -6.05, Min: -99.89, Time: 7.40\n",
            "Episode 27, Average Score: -81.87, Max: -6.05, Min: -99.89, Time: 7.41\n",
            "Episode 28, Average Score: -82.28, Max: -6.05, Min: -99.89, Time: 7.41\n",
            "Episode 29, Average Score: -81.37, Max: -6.05, Min: -99.89, Time: 7.45\n",
            "Episode 30, Average Score: -81.44, Max: -6.05, Min: -99.89, Time: 7.47\n",
            "Episode 31, Average Score: 92.32, Max: 92.32, Min: -99.89, Time: 0.66\n",
            "Model Save...\n",
            "\n",
            "Environment solved in 31 episodes!\tAverage Score: 92.32\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Size of each action: 1\n",
        "\n",
        "Size of state: 2\n",
        "\n",
        "**without Noise**  Environment solved in 21 episodes!\tAverage Score: 50.04\n",
        "\n",
        "**with Noise**  Environment solved in 31 episodes!\tAverage Score: 92.32"
      ],
      "metadata": {
        "id": "bgpILw9RbJ9r"
      }
    }
  ]
}